#### 监督学习

考点：

1. 机器学习基本概念
2. 回归分析
3. <span style="color: red">决策树</span>
4. 线性判别分析
5. 支持向量机

##### 机器学习是从已有的数据中学习知识

过程：

1. 在原始数据中提取特征
2. 学习映射函数f
3. 通过映射函数f将原始数据映射到语义空间，即寻找数据和任务目标之间的关系

![image-20230617200451025](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230617200451025.png)

机器学习的分类：

* 监督学习，数据有标签、一般为回归或者分类等任务
* 无监督学习，数据无标签、一般为聚类或若干降维任务
* 强化学习，序列数据决策学习，一般为与环境交互中学习
* 其中半监督学习是监督学习和无监督学习的混合

监督学习一个应用：分类问题

f（数学好=Yes，会编程=Yes，身材好=....）是一个函数，然后通过条件经过这个函数会得到一个结果，就是归类的结果

##### 监督学习的重要元素

* 标注数据，标识了类别信息的数据，因为监督学习本来就是从数据中获取信息，通过这个信息得到结果，所以要标注数据
* 学习模型，如何学习得到映射模型，学习得到了映射模型，一个输入就可以有一个输出
* 损失函数，如何对学习结果进行度量，损失函数是得到的结果和实际结果的差值，所以要尽可能的减少损失函数

##### 损失函数

* 训练集一共有n和标注数据，第i个标注数据为(xi，yi)，其中第i个样本数据为xi，yi是xi的标注信息
* 从训练集中通过学习得到的映射函数记为f，f与xi的预估结果记为f(xi)。损失函数就是计算xi真实值yi与预测值f(xi)之间差值的函数
* 很显然，在训练过程中希望映射函数在训练数据集上得到 “损失” 之和最小，即从i到n的Loss(f(xi), yi)的求和最小。

##### 常有的损失函数

* 0-1损失函数，如果f(xi) != yi，则Loss=1，否则Loss=0
* 平方损失函数，Loss=(yi - f(xi)) * (yi - f(xi))
* 绝对损失函数，绝对值损失
* 对数损失函数/对数似然损失函数，Loss=-logP((yi | xi))

##### 监督学习：训练数据与测试数据

* 从训练数据集学习得到映射函数f
* 在测试数据集测试映射函数f
* 在未知数据集上测试映射函数f

##### 监督学习：经验风险与期望风险

* 经验风险：训练集中数据产生的损失。经验风险越小说明学习模型对训练数据拟合度越好
* 期望风险：当测试集中存在无穷多数据时产生的损失。期望风险越小，学习所得模型越好

##### 监督学习：“过学习” 与 “欠学习”

| 经验风险小（训练集上表现好） | 期望风险小（测试集上表现好） | 泛化能力强 |
| ---------------------------- | ---------------------------- | ---------- |
| 经验风险小（训练集上表现好） | 期望风险大（测试集上标签不好） | 过学习（模型过于复杂） |
| 经验风险大（训练集上表现不好） | 期望风险大（测试集上标签不好） |欠学习|
| 经验风险大（训练集上表现不好） | 期望风险小（测试集上表现好） |“神仙算法” 或 “黄粱美梦”|

其中第一行中对于 “过学习”，我的理解是把大量的重心均投入到训练集上的拟合，而忽略了位置数据上的测试，才导致经验风险小，期望风险小的情况，所以才导致了 “过学习”，也叫做 “过拟合”。

##### 监督学习两种方法：判别模型和生成模型

监督学习方法又可以分为生成方法和判别方法。所学习得到的模型分别成为生成模型和判别模型。

<strong>判别模型</strong>：

* 判别方法直接学习判别函数f(x)或者条件概率分布p(Y|X)作为预测的模型，即判别模型
* 判别模式关心在给定输入数据下，预测该数据的输出是什么
* 典型判别模型包括回归模型、神经网络、支持向量机和Ada boosting等

<strong>生成模型</strong>

* 生成模型从数据中学习联合概率分布P(x, y)（通过似然概率p(x | y)和类概率P(y)的乘机来获取）
* 典型方法为贝叶斯方法、隐马尔可夫链
* 联合分布概率p(x, y)或似然概率p(x | y)求取很困难

对于判别模型和生成模型，来自知乎大佬的回答：

##### 基本概念

假设我们有训练数据 (x, y)，x是属性集合，y是类别标记。此时来了一个新的样本x，我们想要预测他的类别y。

我们最终的目的是求得最大的条件概率p(y | x)

不管是生成式模型还是判别式模型，他们最终判断依据都是条件概率P(y|x)，但是生成式模型先计算了联合概率P(x, y)，再有贝叶斯公式计算得到条件概率。因此，生成式模型可以体现更多数据本身的分布信息，且普适性更广。

##### 用山羊的例子

判别式：确定一个🐏是一个山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。

生成式：是根据山羊的特征首先学习出一个山羊的模型，根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少

##### 监督学习

训练映射函数f -> 分类、识别、推荐

##### 线性回归

* 在现实生活中，往往需要分析若干变量之间的关系，如碳排放量与气候变暖之间的关系、某一商品广告投入量与该商品销售量之间的关系等等，这种分析不同变量之间存在关系的研究叫做回归分析，刻画不同变量之间关系的模型成为回归模型，如果这个模型是线性的，那就成为线性回归模型。
* 一旦确定了回归模型，就可以进行预测等分析工作，如从碳排放量预测气候变化程度、从广告投入量预测商品销售量等

比如：y = 33.73（英寸） + 0.516x，其中y指的是子女平均身高，x指的是父母平均身高

* 父母平均身高每增加一个单位，其成年子女平均身高只增加0.516个单位，它反映了这种 “衰退” 效应
* 虽然 x 和 y 不总是具有 “衰退” 关系，但是 “线性回归” 这一名称就保留了下来

* 给出x，可以求取y
* 已知y，可以求取x
* 但是如何求取线性方程中的参数呢？

来一个题目：

![image-20230617211215665](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230617211215665.png)

<div style="color: red">Logistic回归模型没有看懂</div>

##### 梯度下降法

几个问题：

* 梯度下降法的结束条件，一般采用：迭代次数达到了最大设定；损失函数降低幅度低于设定的阈值
* 关于步长过大时，初期下降的速度很快，但有可能越过最低点，如果 “洼地” 够大，则会冲过 “洼地”。如果步长够小，则收敛的速度会很慢。因此，可以采用先大后小的策略调整步长，具体大小的调节可以根据f(x)降低的幅度或者x前进的幅度进行

##### 决策树分类算法

  决策树模型是一种对测试样本进行分类的树形结构，该结构由节点 (node) 和有向边 (directed edge) 组成，结点分为内部节点和叶节点两类。内部节点表示对样本的一个特征进行测试，内部节点下面的分支表示该特征测试的输出。如果只对特征的1个具体值进行测试，那么将只有正或负两种输出，生成的是二叉树。如果对特征的多个具体值进行测试，那么产生多个输出，生成的是多叉树。叶子节点表示样本的一个分类，如果样本只有两个分类类别，那么该模型是二分类模型，否则是多分类模型

下面是一个简单的描述 “兔子” 概念的决策树

![image-20230618150633443](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230618150633443.png)

##### 决策树分类算法

决策树学习的基本方法和步骤：
首先，选取一个属性，按这个属性的不同取值对实例集进行划分；并以该属性为根节点，以这个属性的诸多取值进行根节点的分类，进行画树。

然后，考察所得的每一个子类，看其中的实例的结论是否完全相同。如果完全相同，则以这个相同的结论作为相应分支路径末端的叶子节点；否则，选取一个非父节点的属性，按这个属性的不同取值对该子集进行分类，并以该属性作为节点，以这个属性的诸多取值作为节点的分支，继续进行画树。

如此继续，知道所分的子集全部满足：实例结论完全相同，而得到所有的叶子节点为止。

##### 信息量

信息指的是对不确定的消除，消除的不确定越大，那么信息量就越大。

简而言之，信息量指的是该信息包含的价值是多少。如果使用p代表改事件的发生概率，则p越小，信息量就越大

I(x) = -log p

来个例子

比如中国足球队和巴西足球队曾经有过8次比赛，其中中国队胜利1次。以U表示未来中巴比赛中国队胜利的事件，那么U的先验概率就是1/8，因此信息量就是：
I(x) = -log1/8 = 3

如果以T表示巴西队胜利，则T的先验概率就是 7/8，则信息量就是：
I(T) = -log 7/8 = 0.19

##### 信息熵

信息量描述的是信源发出的单个事件消除的不确定性，还不能刻画信源消除的平均不确定性。如果把信源发出的所有事件的信息量求平均值，就可以刻画信源消除的平均不确定性，定义为信息熵：

则有 H(x) = - sum(pi * log pi)

样本集合的信息熵越大，说明各样本相对均衡。

比如：

a) P = (1/4, 1/4, 1/4, 1/4) 此时H(p) = log4

b) P = (1/2, 0, 0, 1/2) 此时H(p) = log2

c) P = (0, 0, 0, 1) 此时H(p) = 0

信息熵越小，表示D包含的信息月确定，也称 D 的纯度越高。

#### 决策树

决策树是一种通过树形结构来进行的方法。在决策树中，树形结构的每一个非叶子节点表示对分裂目标在某个属性上的一个判断，每一个分支代表基于该属性做出的一个判断，每个叶子节点代表一种分类结果，所以决策树可以看作是一系列以叶子节点为输出的决策规则。

![image-20230621092104083](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230621092104083.png)

##### 构建决策树

构建决策树时划分属性的顺序是重要的。性能好的决策树随着划分不断进行，决策树分支节点样本集的 “纯度” 会越来越高，即其所有包含样本尽可能属于相同类别。

