#### 强化学习

##### 强化学习中的概念

* 智能体：智能体是强化学习算法的主体，它能根据经验做出主观判断并执行动作，是整个智能系统的核心
* 环境：智能体以外的一切统称为环境，环境在于智能体交互中，能被智能体所采取的动作影响，同时环境也能向智能体反馈状态和奖励
* 状态：状态可以理解为智能体对环境的一种理解和编码，通常包含了对智能体所采取决策产生影响的信息
* 动作：动作是智能体对环境产生影响的方式
* 策略：策略是智能体在所处状态下去执行某个动作的依据，即给定一个状态，智能体可根据一个决策来选择应该采取的动作
* 奖励：奖励是智能体序贯式采取一系列动作后从环境中获取的利益

强化学习的特点：

![image-20230620201129036](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230620201129036.png)

可以看到选择能够获取最大收益的状态到动作的映射。

* 基于评估：强化学习利用环境评估当前策略，依次为依据进行优化
* 交互性：强化学习的数据是在于环境的交互中产生
* 序列决策过程：智能主体在于环境的交互中需要做出一系列决策，这些决策往往都是前后关联的

注：现实中强化学习问题往往还存在奖励滞后，基于采样的评估等特点

由此可以看出，这个智能体更像我们所预期的那样智能。

##### 马尔可夫决策过程

![image-20230620203317982](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230620203317982.png)

这个公式可知：价值函数取值与时间没有关系，只与策略、在策略下状态转移到其后续状态所取得的回报以及在后续所得回报有关。

进一步分析，课件状态s取得好处由两部分构成：一是在状态s执行当前动作所得到的顺时奖励，另一个在后续状态所能的回报期望的折扣值。

相比于关心每个状态的价值，显然关心在当前状态下施以某个动作带来的价值更加直观实用。

强化学习会寻找一个最优策略，在策略的作用下使得任意状态对应的价值函数取得最大。

![image-20230620203823309](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230620203823309.png)

图中展示了一种思路：从一个任意的策略开始，首先计算该策略下价值函数（或动作-价值函数），根据价值函数调整改进策略使其更优，不断迭代这个过程直到策略收敛。通过策略计算价值函数的过程成为策略评估，通过价值函数优化策略的过程叫做策略优化，策略评估和策略优化交替形成强化学习求解的方法叫做策略迭代。

