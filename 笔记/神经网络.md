#### 神经网络

#### 前馈神经网络

前馈神经网络作为最基础的神经网络，一般包括输入层、隐藏层和输出层。每个神经元只和邻层神经元相连，即每层神经元只接受相邻前序神经元所传来的信息，只给相邻后续神经层中神经元传递信息。在前馈神经元网络中，同一层神经元没有任何链接，后续神经元不向其前序相邻神经层传递任何信息。前馈神经网络是目前应用最为广泛的神经网络之一。

#### 若干概念

在正式介绍前馈神经元网络之前，先介绍神经元、激活函数、输出单元以及损失函数等概念

##### 神经元

生物学中，神经元细胞有兴奋与抑制两种状态。大多数神经元细胞在正常情况下处于抑制状态，一旦神经元受到刺激并且电位超过一定的阈值后，这个神经元细胞就被激活，处于兴奋状态，并向其他神经元传递信息。基于神经元细胞的结构特征与传递信息方式，神经科学家和逻辑学家提出了MCP模型，

#### 梯度下降算法

分为批量梯度下降算法、随机梯度下降算法、小批量梯度下降算法等类型

* 批量下降算法是在整个训练集上处理损失误差L(θ)。如果数据集较大，则可能出现内存容量不足而无法完成，另一方面收敛速度比较慢。
* 随机梯度下降算法是使用训练集中每个训练样本计算所得L(θ)来分别更新参数，可见速度要快一点，但是可能会出现所优化目标函数震荡不稳定现象。
* 小批量梯度下降算法是选取训练集中小批量计算L(θ)，根据每一批样本所得到的累加误差来更新参数，这样可以保证训练过程更稳定，而且采用小批量训练方法也可利用矩阵计算优势。

#### 卷积神经网络

##### 步骤一：卷积计算

##### 步骤二：池化：

由于图像中存在较多冗余，在图像处理中，可用某一区域子块的统计信息（如最大值或均值）来刻画该区域中所有像素点呈现的空间分布模式，以代替区域字块中所有像素点取值，这就是卷积神经网络中的池化操作。

* 最大池化
* 平均池化
* K最大池化

池化操作是一种下采样操作，是对信息进行约简和抽象的过程，可简化卷积神经网络计算的复杂度，保持特征的某种不变性（选择、平移和伸缩等），以增强神经网络所学习特征的鲁棒性（防止过拟合和过学习）。

基于卷积与池化操作，就可以定义一个完整的卷积神经网络。

比如下面的图：
![image-20230620193445477](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230620193445477.png)

* 对于32 * 32 * 3 的彩色图像，先用6个5 * 5 * 3的卷积核对其进行卷积操作（步长为1、无填充），得到6个28 * 28的卷积结果
* 然后以2 * 2进行池化，得到6个14 * 14大小的特征图
* 最后通过若干个全连接层得到原始图像的向量表达
* 再对这个向量表达通过 softmax 函数进行分类识别，得到输入图像隶属于n个类别的概率

##### 神经网络正则化

为了缓解神经网络出现的过拟合问题，需要采取一些正则化技术来提升神经网络的泛化能力。

* Dropout：Dropout指的是在训练神经网络过程中随即丢掉一部分神经元来减少神经网络复杂度，为了防止过拟合。
* 批归一化：批归一化激素hi通过规范化手段，把神经网络每层中任意神经元的输入值分布改为均值为0，方差为1的标准正态分布，把偏移较大的分布强制映射为标准正态分布。通过批归一化处理，激活函数的输入值被映射到非线性函数梯度较大的区域，使得梯度变大从而克服梯度消失问题，进而提高收敛速度。
* L1和L2正则化。
