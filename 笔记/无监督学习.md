#### 无监督学习

##### kmeans 算法

此部分略过，比较简单

##### 主成分分析

* 主成分分析是一种特征降维方法。人类在认知过程中会主动 “化繁为简”
* 奥卡姆剃刀定律：“如无必要，勿增实体”，即 “简单有效原理”

##### 若干概念：方差与协方差

* 方差等于各个数据与样本均值只差的平方和的平均数
* 方差描述了样本数据的波动程度

![image-20230621004501427](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230621004501427.png)

* 协方差用于衡量两个变量之间的相关度

![image-20230621004546677](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230621004546677.png)

##### 结论：

* 当协方差𝑐𝑜𝑣 𝑋, 𝑌 > 0 时，称𝑋 与𝑌 正相关 
* 当协方差𝑐𝑜𝑣 𝑋, 𝑌 < 0 时，称𝑋 与𝑌 负相关
* 当协方差𝑐𝑜𝑣 𝑋, 𝑌 = 0 时，称𝑋 与𝑌 不相关（线性意义下）

##### 主成分分析：从协方差到相关系数

![image-20230621004713232](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230621004713232.png)

主要性质：

* |𝑐𝑜𝑟𝑟 𝑋, 𝑌 | ≤ 1
* 𝑐𝑜𝑟𝑟 𝑋, 𝑌 = 1的充要条件是存在常数𝑎和𝑏,使得𝑌 = 𝑎𝑋 + 𝑏
* 皮尔逊相关系数是对称的，即𝑐𝑜𝑟𝑟 𝑋, 𝑌 = 𝑐𝑜𝑟𝑟 𝑌, 𝑋 
* 由此衍生出如下性质：皮尔逊相关系数刻画了变量𝑋和𝑌之间线性相关程度，如果|𝑐𝑜𝑟𝑟 𝑋, 𝑌 | 的取值越大，则两者在线性相关的意义下相关程度越大。 𝑐𝑜𝑟𝑟 𝑋, 𝑌 = 0表示两者不存在线 性相关关系（可能存在其他非线性相关的关系）。
* 正线性相关意味着变量𝑋增加的情况下，变量𝑌也随之增加；负线性相关意味着变量𝑋减少的 情况下，变量𝑌随之增加。

##### 相关性与独立性

* 如果𝑿和𝒀的线性不相关，则 𝑐𝑜𝑟𝑟 𝑋, 𝑌 = 0 
* 如果𝑿和𝒀的彼此独立，则一定 𝑐𝑜𝑟𝑟 𝑋, 𝑌 = 0，且𝑿和𝒀不存在任何线性或非线性关系 
* “不相关”是一个比“独立”要弱的概念，即独立一定不相关，但是不相关不一定相互 独立（可能存在其他复杂的关联关系）。独立指两个变量彼此之间不相互影响。

##### 主成分分析：算法动机

* 在数理逻辑中，方差经常被用来度量数据和其数学期望之间的偏离程度，这个偏离程度反映了数据分布结构
* 在很多实际问题中，研究数据和其均值之间的偏离程度有着很重要的意义
* 在降维之后，需要尽可能将数据向方差最大方向进行投影，使得数据所蕴含信息没有丢失，彰显个性。如左下图所示，向𝑦方向投影（使得二维数据映射为一维）就比向𝑥方向投影结果 在降维这个意义上而言要好；右下图则是黄线方向投影要好。

![image-20230621005232907](C:\Users\hp   Ming\AppData\Roaming\Typora\typora-user-images\image-20230621005232907.png)

* 主成分分析思想就是将n维特征数据映射到 l 维空间 (n >= l)，去除原始数据之间的冗余性
* 将原始数据向这些数据方差最大的方向进行投影。一旦发现了方差最大的投影方向，则继续寻找保持方差第二的方向且进行投影
* 将每个数据从n维高位空间映射到l维低维空间，每个数据所得到最好的k维特性就是使得每一维样本方差都尽可能大

